version: '3.2'

services:
  zookeeper:
    image: bitnami/zookeeper:latest
    ports:
      - '2181:2181'
    volumes:
      - ./zookeeper/data:/bitnami/zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: bitnami/kafka:latest
    
    restart: unless-stopped
    ports:
      - '9092:9092'
      - 29092:29092
    volumes:
      - ./kafka/data:/bitnami/kafka
    environment:
      - KAFKA_ADVERTISED_HOST_NAME=localhost
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS= PLAINTEXT://localhost:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP= PLAINTEXT:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CREATE_TOPICS="test:1:1"
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
    depends_on:
      - zookeeper
  connect:
    image: confluentinc/cp-kafka-connect
    ports:
    - '8083:8083'
    - '9093:9092'
    environment:
        - CONNECT_BOOTSTRAP_SERVERS=kafka:9092
        - CONNECT_GROUP_ID=kafcon3xx
        - CONNECT_CONFIG_STORAGE_TOPIC=_kafcon3xx-config
        - CONNECT_OFFSET_STORAGE_TOPIC=_kafcon3xx-offsets
        - CONNECT_STATUS_STORAGE_TOPIC=_kafcon3xx-status
        - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR= 1
        - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR= 1
        - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR= 1
        - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
        - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
        - CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
        - CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
        - CONNECT_REST_ADVERTISED_HOST_NAME=localhost
        - CONNECT_PLUGIN_PATH=./kafka-connect/data
        - ZOOKEEPER_CLIENT_PORT=2181
              # Logging
      # Connect client overrides
        - CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS= 30000
        - CONNECT_OFFSET_FLUSH_INTERVAL_MS= 900000
        - CONNECT_CONSUMER_MAX_POLL_RECORDS= 500
    depends_on:
    - zookeeper
    - kafka

  postgres:
    build:
      context: postgres/
    volumes:
      - ./postgres/db:/var/lib/postgresql/data
    ports:
      - 5432:5432
    environment:
      
      DB_HOST: db
      DB_PORT: 5432
      DB_USER: postgres
      DB_PASSWORD: postgres
      DB_NAME: GOGREEN
      POSTGRES_PASSWORD: postgres

  redis:
    build:
      context: redis/
    volumes:
      - ./redis/data:/bitnami/redis/data
    ports:
      - "6379:6379"
    environment:
      - ALLOW_EMPTY_PASSWORD=yes

  rabitmq:
    build:
      context: rabitmq/
    volumes:
      - ./rabbitmq/data/:/var/lib/rabbitmq/
      # - /rabbitmq/log/:/var/log/rabbitmq
    ports:
      - 5672:5672
      - 15672:15672
    environment:
      RABBITMQ_ERLANG_COOKIE: "SWQOKODSQALRPCLNMEQG"
      RABBITMQ_DEFAULT_USER: "rabbitmq"
      RABBITMQ_DEFAULT_PASS: "rabbitmq"
      RABBITMQ_DEFAULT_VHOST: "/"

  elasticsearch:
    build:
      context: elasticsearch/
      args:
        ELK_VERSION: $ELK_VERSION
    volumes:
      - type: bind
        source: ./elasticsearch/config/elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
        read_only: true
      - type: volume
        source: elasticsearch
        target: /usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx512m -Xms512m"
      # Use single node discovery in order to disable production mode and avoid bootstrap checks.
      # see: https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html
      discovery.type: single-node
    networks:
      - elk

  logstash:
    build:
      context: logstash/
      args:
        ELK_VERSION: $ELK_VERSION
    volumes:
      - type: bind
        source: ./logstash/config/logstash.yml
        target: /usr/share/logstash/config/logstash.yml
        read_only: true
      - type: bind
        source: ./logstash/pipeline
        target: /usr/share/logstash/pipeline
        read_only: true
      - type: bind
        source: ./logstash/logs
        target: /home/logs
        read_only: true
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - elk
    depends_on:
      - elasticsearch

  kibana:
    build:
      context: kibana/
      args:
        ELK_VERSION: $ELK_VERSION
    volumes:
      - type: bind
        source: ./kibana/config/kibana.yml
        target: /usr/share/kibana/config/kibana.yml
        read_only: true
    ports:
      - "5601:5601"
    networks:
      - elk
    depends_on:
      - elasticsearch
networks:
  elk:
    driver: bridge

volumes:
  elasticsearch:
